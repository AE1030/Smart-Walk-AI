The final prototype is a walking cane with a built-in AI used to detect everyday objects. The team incorporated a YOLO AI algorithm from Ultralytics as the model. YOLO models are known for their incredibly fast processing speeds, which are very important for real-time object detection [16]. The model was trained on a refined COCO (Common Objects in Context) dataset containing approximately 123k images across all categories [15]. The model typically comes pre-trained, but since there were other objects that needed to be detected, additional datasets were added to COCO. 

Now that we had our AI model, a filtering algorithm needed to be implemented to ensure the output of the objects was clear and precise, and it's not just outputting all the objects the AI model was picking up at random. It is important to note that out of the 80+ categories, it was decided that only 30 will make it to the output stage. 

The first problem encountered was the AI model spamming the same object repeatedly. To fix this, a cooldown time of 25 seconds was added — meaning if a specific object was already detected, it couldn’t be detected again for another 25 seconds. This allowed for other objects to be detected as well. 

Another problem encountered was the model’s tendency to hallucinate, which is inevitable when using AI. An efficient and easy way to address this is to add logic that doesn’t allow output if the model’s confidence score is below a certain threshold (0.65). This works well if the model is well trained and hallucinates from time to time. If it happens often, then there is either a problem with the dataset used or a problem during the training process.  To further ensure accuracy, an algorithm was added to confirm whether the object was really there by having the model validate it in at least 2 out of 5 frames before approving it for output. 

The last challenge encountered on the coding side was outputting the approximate distance of the object after it had been detected and located. The formula used to get this is shown below, but again, it’s just a rough approximation. To get more accurate distance calculations, a camera with depth perception is needed. This wasn’t a huge concern, as initially the team never set out to calculate the distance of the object relative to the client, but it helps optimize the filtering algorithm by only detecting relevant objects. 

To get the position of the detected object (whether it is to the left, right, or straight ahead) relative to the client, a comparison was made between the width of the screen to the width of the bounding boxes drawn around the detected object. Using this, we can accurately predict where the object is in space (a full breakdown of the algorithm is explained below). The output information is delivered through Python’s built-in text-to-speech library (pyttsx3). 

In terms of hardware, a Raspberry Pi 4 is used to run the program, which connects to a Google Coral and a webcam through the USB 3.0 ports, and to the battery bank through the USB-C port. The Google Coral is designed to run machine learning algorithms at accelerated speeds, requiring less processing to be done by the Raspberry Pi’s CPU [17]. These hardware components didn’t need to be purchased, as one of the team members already had them. 

In the future, the team aspires to add more features and datasets to the AI model to identify a wider range of objects. Future work will involve improving distance calculations by using a camera with depth perception. As for the current testing plan, adding accurate distance calculation as one of the objectives will help with implementing a proper solution. To test this objective, a measurement of the actual distance between the client and the object can be compared to the distance returned by the program. If the error is consistently below a certain threshold, then the objective will be considered achieved. 
