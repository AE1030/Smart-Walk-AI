Our final prototype is a walking cane with a built-in AI used to detect everyday objects. We used the YOLO AI algorithm from Ultralytics as our model. YOLO models are known for their incredibly fast processing speeds, which are very important for real-time object detection. The model was trained on a refined COCO (Common Objects in Context) dataset containing approximately 123k images across all categories. The model typically comes pre-trained, but since there were other objects we wanted to detect outside of just the categories found in the COCO dataset, we added another set of categories (trash, motorcycle, tree, crosswalk).

Now that we had our AI model, we needed to come up with a filtering algorithm to ensure the output of the objects was clear and precise, and we weren’t just outputting all the objects the AI model was picking up at random. It is important to note that out of the 80+ categories we had, we only decided to proceed with around 30.

The first problem we encountered was the AI model spamming the same object repeatedly. To fix this, we added a cooldown time of 25 seconds — meaning if a specific object was already detected, it couldn’t be detected again for another 25 seconds. This allowed for other objects to be detected as well.

Another problem encountered was our model’s tendency to hallucinate, which is inevitable when using AI. An efficient and easy way to address this was to add logic that doesn’t allow output if the model’s confidence score is below a certain threshold (in our case, 0.65). This works well if you have a well-trained model that hallucinates from time to time. If it happens often, then there is either a problem with the dataset used or a problem during the training process — but for us, this was not the case.

We also added an algorithm to confirm whether the object was really there by having the model validate it in 2 out of 5 frames before approving it for output.

The last challenge we encountered on the coding side was outputting the approximate distance of the object after it had been detected and located. The formula used to get this is shown below, but again, it’s just a rough approximation. To get more accurate distance calculations, we would need a camera with depth perception. This wasn’t a huge concern for us, as we never set out to calculate the distance of the object relative to the client, but it helped us optimize our filtering algorithm by only detecting relevant objects.

To get the position of the detected object (whether it is to the left, right, or straight ahead) relative to the client, we used the width of the screen and compared it to the width of the bounding boxes drawn around the detected object. Using this, we can accurately predict where the object is in space (a full breakdown of the algorithm is explained below). The output information is delivered through Python’s built-in text-to-speech library (pyttsx3).

The algorithms explained above for distance calculations and model hallucination came after the first testing session, as we were noticing inaccurate predictions and objects being very far yet still being returned. This allowed us to meet the accurate object detection objective we set out to achieve and test in Milestone 5.

During the first testing plan, we also noticed that the Raspberry Pi wasn’t receiving the proper voltage (5V) due to the power bank used — it was bad at regulating voltage and had a low capacity of 10,000mAh. We decided to switch to an INUI power bank with a capacity of 25,000mAh, significantly boosting the battery life. This allowed us to meet our efficient battery life objective, as the battery now lasts for 8 hours and 20 minutes.

This also worked in our favor, as it forced us to change our design to make it more lightweight. This helped us achieve the weight objective, as the overall mass remained relatively light compared to the cane. Originally, we had the power bank in the components box, but since we were forced to use an alternative, we decided to carry it in a small fanny pack connected to the Raspberry Pi via a waterproof USB-C cable. This is an example of how the testing plan allowed for a change in the design, enabling us to confidently hit our lightweight design objective.

In terms of hardware, we are using a Raspberry Pi 4 to run the program, which connects to a Google Coral and a webcam through the USB 3.0 ports, and to the battery bank through the USB-C port. The Google Coral is designed to run machine learning algorithms at accelerated speeds, requiring less processing to be done by the Raspberry Pi’s CPU. These hardware components didn’t need to be purchased, as one of our members already had them, allowing us to stay well within budget.

The material used was tough PLA — a specific type of PLA designed for high-impact resistance. Tough PLA has twice the impact resistance of regular PLA, which is useful for scenarios where the client drops the device [5][6]. Since the device is 3D printed, there are very few or no pores in the casing, which allows minimal water entry. This was proven during weather testing: rain was simulated on the prototype, and it continued to function as normal, proving the device's durability. The prototype was also dropped to ensure durability, and it continued to work well.

To demonstrate our integration with existing walking canes, we used a common cane which cost around $18, ensuring the cost remained under our constraint. The mechanism was also tested by five individuals who imitated client conditions by closing their eyes. They ranked ease of use on a scale of 1 to 5 (with 5 being the easiest), and the average rating was 4.2/5 — showing that this device is relatively easy to adapt to.

In the future, we hope to add more features and datasets to our model to identify a wider range of objects. We also plan to improve distance calculations by using a camera with depth perception. As for the current testing plan, we can refine it by adding accurate distance calculation as one of the objectives. To test this, we can measure the actual distance between the client and the object and compare it to the distance returned by our program. If the error is consistently below a certain threshold, then the objective will be considered achieved.
